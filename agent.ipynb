{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bf5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool,AgentExecutor\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.tools.file_management.read import ReadFileTool\n",
    "from langchain.tools.file_management.write import WriteFileTool\n",
    "from langchain.tools.file_management.list_dir import ListDirectoryTool\n",
    "from langchain.tools.file_management.copy import CopyFileTool\n",
    "from langchain.tools.file_management.move import MoveFileTool\n",
    "from langchain.tools.file_management.delete import DeleteFileTool\n",
    "from langchain_community.tools import ShellTool\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63da86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDG1IRqNGLbK5Fi5amvKlOHRBWulI5ANQc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d849e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_correction = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=\"AIzaSyCDZh_90GVed3iyvfYsVRNI2exTYRsWHx0\"\n",
    ")\n",
    "\n",
    "llm_testing = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=\"AIzaSyCDZh_90GVed3iyvfYsVRNI2exTYRsWHx0\"\n",
    ")\n",
    "\n",
    "llm_mapping = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=\"AIzaSyCDZh_90GVed3iyvfYsVRNI2exTYRsWHx0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01fa6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_tool = ShellTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a9dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_repl = PythonREPL()\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a47f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [\n",
    "    ReadFileTool(),\n",
    "    WriteFileTool(),\n",
    "    ListDirectoryTool(),\n",
    "    CopyFileTool(),\n",
    "    MoveFileTool(),\n",
    "    DeleteFileTool(),\n",
    "    repl_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "564253da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bitcount',\n",
       " 'breadth_first_search',\n",
       " 'bucketsort',\n",
       " 'depth_first_search',\n",
       " 'detect_cycle',\n",
       " 'find_first_in_sorted',\n",
       " 'find_in_sorted',\n",
       " 'flatten',\n",
       " 'gcd',\n",
       " 'get_factors',\n",
       " 'hanoi',\n",
       " 'is_valid_parenthesization',\n",
       " 'kheapsort',\n",
       " 'knapsack',\n",
       " 'kth',\n",
       " 'lcs_length',\n",
       " 'levenshtein',\n",
       " 'lis',\n",
       " 'longest_common_subsequence',\n",
       " 'max_sublist_sum',\n",
       " 'mergesort',\n",
       " 'minimum_spanning_tree',\n",
       " 'next_palindrome',\n",
       " 'next_permutation',\n",
       " 'node',\n",
       " 'pascal',\n",
       " 'possible_change',\n",
       " 'powerset',\n",
       " 'quicksort',\n",
       " 'reverse_linked_list',\n",
       " 'rpn_eval',\n",
       " 'shortest_paths',\n",
       " 'shortest_path_length',\n",
       " 'shortest_path_lengths',\n",
       " 'shunting_yard',\n",
       " 'sieve',\n",
       " 'sqrt',\n",
       " 'subsequences',\n",
       " 'topological_ordering',\n",
       " 'to_base',\n",
       " 'wrap',\n",
       " '__pycach']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog_name_list=os.listdir(\"python_programs\")\n",
    "new=[]\n",
    "new1=[]\n",
    "for i in prog_name_list:\n",
    "    new.append(i[ :-3])\n",
    "for x in new:\n",
    "\n",
    "    if x[-4: ]==\"test\":\n",
    "        continue\n",
    "    else:\n",
    "        new1.append(x)\n",
    "prog_name_list=new1\n",
    "prog_name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaryaman Bisht\\AppData\\Local\\Temp\\ipykernel_22388\\4291636008.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_correction = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "agent_correction = initialize_agent(\n",
    "    tools=tools_list,\n",
    "    llm=llm_correction,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "agent_correction_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent_correction.agent,\n",
    "    tools=tools_list,\n",
    "    verbose=True,\n",
    "    max_iterations=1000,          \n",
    "    max_execution_time=3000     \n",
    ")\n",
    "\n",
    "agent_testing = initialize_agent(\n",
    "    tools=[],\n",
    "    llm=llm_testing,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "agent_testing_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent_testing.agent,\n",
    "    tools=[],\n",
    "    verbose=True,\n",
    "    max_iterations=1000,           \n",
    "    max_execution_time=3000      \n",
    ")\n",
    "\n",
    "agent_mapping = initialize_agent(\n",
    "    tools=tools_list,\n",
    "    llm=llm_mapping,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "agent_mapping_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent_mapping.agent,\n",
    "    tools=tools_list,\n",
    "    verbose=True,\n",
    "    max_iterations=1000,        \n",
    "    max_execution_time=3000      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ceb6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_with_retry(agent_executor, prompt, max_retries=6):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return agent_executor.run(prompt)\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) or \"ResourceExhausted\" in str(e):\n",
    "                print(f\"⚠️ Quota hit. Waiting 60 seconds before retrying... (Attempt {attempt + 1})\")\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                raise e\n",
    "    raise RuntimeError(\"All retries failed due to quota exhaustion or other errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was previous approach with general prompt, which was giving json parsing error in llm output.\n",
    "#So i decided to use Prompt Template.\n",
    "\n",
    "\n",
    "\n",
    "# # prog_name=input(\"Enter the name of the program to be fixed:\")\n",
    "\n",
    "# for prog_name in prog_name_list:\n",
    "#    fixed=0\n",
    "#    messages=[]\n",
    "#    while(fixed==0):\n",
    "#       correction_prompt = f\"\"\"\n",
    "#       You are an expert Python bug fixer. Read all Python file located at: \"python_programs\".\n",
    "     \n",
    "#       the name of python file is {prog_name}\n",
    "\n",
    "#       Follow these steps:\n",
    "#       1. Read the file.\n",
    "#       2. Identify the bug in the code and classify it as one of the 14 QuixBugs error types:\n",
    "#          [\"Conditional block omission\", \"Expression modification\", \"Comparison logic error\", \"Loop boundary error\", \n",
    "#          \"Operator omission\", \"Wrong operator\", \"Incorrect variable initialization\", \"Off-by-one error\", \n",
    "#          \"Loop condition logic\", \"Incorrect return statement\", \"Wrong function call\", \"Exception handling bug\", \n",
    "#          \"Variable misuse\", \"Incorrect function condition\"]\n",
    "\n",
    "#       3. Fix the bug.\n",
    "#       4. Add a Python comment at the top stating: `# Bug Type: <classified_bug_type>`\n",
    "#       5. Save the corrected file to: \"fixed_programs/{prog_name}.py\"\n",
    "#       Respond ONLY with a valid JSON object, and no other text.\n",
    "#       Make sure all special characters (like backslashes) in strings are properly escaped. Use double-escaped backslashes (\\\\) and escape quotes inside strings.\n",
    "#       Return your response strictly as a JSON object with keys: `action` and `action_input`. Do NOT include markdown code blocks (no ```).\n",
    "\n",
    "\n",
    "#       \"\"\"\n",
    "#       correction_output=run_with_retry(agent_correction_executor, correction_prompt)\n",
    "#       messages.append(correction_output)\n",
    "\n",
    "#       if(prog_name==\"knapsack\"):\n",
    "#          test_cmd=shell_tool.run(f\"pytest --fixed --runslow python_testcases/test_{prog_name}.py\")\n",
    "#       else:\n",
    "#          test_cmd=shell_tool.run(f\"pytest --fixed python_testcases/test_{prog_name}.py\")\n",
    "\n",
    "\n",
    "#       testing_prompt=f\"\"\"You are a Python testing assistant.\n",
    "\n",
    "#       The following is the output from running tests on a Python program using a shell command:\n",
    "\n",
    "#       {test_cmd}\n",
    "\n",
    "#       Instructions:\n",
    "#       1. Carefully read the test output.\n",
    "#       2. Determine if all the tests have passed.\n",
    "#          - If **all tests pass** (e.g., no errors, no failed assertions, all tests green), then respond with exactly:\n",
    "#          ```\n",
    "#          correct\n",
    "#          ```\n",
    "#          - If **any test fails**, even one, or there are errors in execution, then respond with exactly:\n",
    "#          ```\n",
    "#          wrong\n",
    "#          ```\n",
    "\n",
    "#       Your output should be only one word: either `correct` or `wrong`.\n",
    "#       Respond ONLY with a valid JSON object, and no other text.\n",
    "#       Make sure all special characters (like backslashes) in strings are properly escaped. Use double-escaped backslashes (\\\\) and escape quotes inside strings.\n",
    "#       Return your response strictly as a JSON object with keys: `action` and `action_input`. Do NOT include markdown code blocks (no ```).\n",
    "\n",
    "\n",
    "#       \"\"\"\n",
    "#       testing_output=run_with_retry(agent_testing_executor, testing_prompt)\n",
    "\n",
    "#       if testing_output==\"correct\":\n",
    "#          fixed=1\n",
    "#       elif testing_output==\"wrong\":\n",
    "#          fixed=0\n",
    "   \n",
    "#    mapping_prompt=f\"\"\"You are an expert bug analysis assistant.\n",
    "\n",
    "#    You will be given a list of bug-fixing logs for Python programs. Your job is to extract mappings between bug types and their corresponding fixes.\n",
    "\n",
    "#    Each log contains a Python comment like: `# Bug Type: <TYPE>`. Use that to detect the bug type.\n",
    "\n",
    "#    For each bug type found, output:\n",
    "#    - The **exact bug type** (must match one of the 14 known QuixBugs types).\n",
    "#    - A **one-line explanation** of what kind of fix was applied to resolve it, based on the log content.\n",
    "\n",
    "#    Use this format for each mapping:\n",
    "#    Bug Type: <TYPE>\n",
    "#    Fix: <one-line explanation of the fix>\n",
    "#    Bug Type:Fix\n",
    "#    Enter this a .txt file in the root folder named 'Bugs_Mapping.txt'. \n",
    "#    And always append into this file.\n",
    "\n",
    "#    If analyzing one program you encounter a bug already in the Bugs_Mapping.txt, update the fix whle retaining the previous fix but also improve it by having the new fix used in the program.\n",
    "#    So in the Bugs_Mapping.txt fle 1 of the 14 bugs only apppears once.\n",
    "#    Respond ONLY with a valid JSON object, and no other text.\n",
    "#    Make sure all special characters (like backslashes) in strings are properly escaped. Use double-escaped backslashes (\\\\) and escape quotes inside strings.\n",
    "#    Return your response strictly as a JSON object with keys: `action` and `action_input`. Do NOT include markdown code blocks (no ```).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#    Logs to analyze:\n",
    "#    {''.join(messages)}\n",
    "#    \"\"\"\n",
    "#    run_with_retry(agent_mapping_executor, mapping_prompt)\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc16590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought:Okay, I will read the file, identify the bug, classify it, fix it, add a comment, and save the corrected file.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"read_file\",\n",
      "  \"action_input\": {\n",
      "    \"file_path\": \"python_programs/depth_first_search.py\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mdef depth_first_search(startnode, goalnode):\n",
      "    nodesvisited = set()\n",
      " \n",
      "    def search_from(node):\n",
      "        if node in nodesvisited:\n",
      "            return False\n",
      "        elif node is goalnode:\n",
      "            return True\n",
      "        else:\n",
      "            return any(\n",
      "                search_from(nextnode) for nextnode in node.successors\n",
      "            )\n",
      "\n",
      "    return search_from(startnode)\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "Depth-first Search\n",
      "\n",
      "\n",
      "Input:\n",
      "    startnode: A digraph node\n",
      "    goalnode: A digraph node\n",
      "\n",
      "Output:\n",
      "    Whether goalnode is reachable from startnode\n",
      "\"\"\"\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mOkay, I have read the file. The code implements a depth-first search algorithm. The bug might be related to the `nodesvisited` set not being updated, leading to infinite recursion if there are cycles in the graph. The correct behavior should mark the current node as visited *before* exploring its successors. This prevents infinite loops. The bug type is \"Conditional block omission\" as we are missing adding nodes to the visited set.\n",
      "\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"write_file\",\n",
      "  \"action_input\": {\n",
      "    \"file_path\": \"fixed_programs/depth_first_search.py\",\n",
      "    \"text\": \"# Bug Type: Conditional block omission\\ndef depth_first_search(startnode, goalnode):\\n    nodesvisited = set()\\n\\n    def search_from(node):\\n        if node in nodesvisited:\\n            return False\\n        elif node is goalnode:\\n            return True\\n        else:\\n            nodesvisited.add(node)\\n            return any(\\n                search_from(nextnode) for nextnode in node.successors\\n            )\\n\\n    return search_from(startnode)\\n\",\n",
      "    \"append\": false\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mFile written successfully to fixed_programs/depth_first_search.py.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The bug is fixed. The file is saved to fixed_programs/depth_first_search.py\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Executing command:\n",
      " pytest --fixed python_testcases/test_depth_first_search.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aaryaman Bisht\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\tools\\shell\\tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought:The test results show that all 5 tests passed.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"correct\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought:I need to extract the bug type and a one-line fix explanation from the provided logs and store them in a file named 'Bugs_Mapping.txt'. Since the file might already exist, I need to append to it and update the fix if the bug type already exists. In this case, there is no bug type specified in the log. Therefore, I will write a default entry to the file.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"write_file\",\n",
      "  \"action_input\": {\n",
      "    \"file_path\": \"Bugs_Mapping.txt\",\n",
      "    \"text\": \"- Bug Type: Unknown\\n- Fix: Bug type not specified in the log.\\n\",\n",
      "    \"append\": true\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mFile written successfully to Bugs_Mapping.txt.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have written a default entry to the Bugs_Mapping.txt file because the provided log did not contain a specific bug type. Now, I will respond with the final answer.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Wrote default entry to Bugs_Mapping.txt because no bug type was specified in the log.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Wrote default entry to Bugs_Mapping.txt because no bug type was specified in the log.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Updated Correction PromptTemplate\n",
    "correction_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are an expert Python bug fixer. Respond ONLY with a valid JSON object, and no other text. \"\n",
    "        \"Format: {{'action': 'write_file', 'action_input': {{'file_path': ..., 'text': ..., 'append': false}}}}. \"\n",
    "        \"Make sure all special characters are escaped properly. Do NOT include markdown code blocks (no ```).\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"You are given a Python file located at: \"python_programs/{prog_name}.py\".\n",
    "        \n",
    "Previous Fix Attempts:\n",
    "{messages}\n",
    "\n",
    "Follow these steps:\n",
    "1. Read the file.\n",
    "2. Consider the above fix attempts and your past changes.Retain the general structure of code. All are one liner fixes.\n",
    "3. Identify the bug and classify it as one of the 14 QuixBugs error types:\n",
    "   [\"Conditional block omission\", \"Expression modification\", \"Comparison logic error\", \"Loop boundary error\", \n",
    "   \"Operator omission\", \"Wrong operator\", \"Incorrect variable initialization\", \"Off-by-one error\", \n",
    "   \"Loop condition logic\", \"Incorrect return statement\", \"Wrong function call\", \"Exception handling bug\", \n",
    "   \"Variable misuse\", \"Incorrect function condition\"]\n",
    "4. Fix the bug.\n",
    "5. Add a Python comment at the top stating: # Bug Type: <classified_bug_type>\n",
    "6. Save the corrected file to: fixed_programs/{prog_name}.py\n",
    "\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "# Testing PromptTemplate\n",
    "testing_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a Python testing assistant. Respond ONLY with a valid JSON object, and no other text. \"\n",
    "        \"Format: {{'action': 'mark_result', 'action_input': {{'result': 'correct' or 'wrong'}}}}. \"\n",
    "        \"Make sure all special characters are escaped properly. Do NOT include markdown code blocks (no ```).\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"The following is the output from running tests on a Python program:\n",
    "\n",
    "{test_cmd}\n",
    "\n",
    "Instructions:\n",
    "- If **all tests pass**, respond exactly: correct\n",
    "- If **any test fails**, respond exactly: wrong\n",
    "Respond with just one word: correct or wrong.\n",
    "\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Mapping PromptTemplate\n",
    "mapping_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are an expert bug analysis assistant. Respond ONLY with a valid JSON object, and no other text. \"\n",
    "        \"Make sure all special characters are escaped properly. Do NOT include markdown code blocks (no ```).\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"You will be given a list of bug-fixing logs for Python programs.\n",
    "Each log contains a Python comment like: `# Bug Type: <TYPE>`. Use that to detect the bug type.\n",
    "\n",
    "For each bug type found, output:\n",
    "- Bug Type: <TYPE>\n",
    "- Fix: <one-line explanation>\n",
    "\n",
    "Enter this in a file named 'Bugs_Mapping.txt' in the root folder.\n",
    "Always append, but update/improve existing fix if bug already exists.\n",
    "\n",
    "Logs to analyze:\n",
    "{logs}\n",
    "\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Loop through all programs\n",
    "# for prog_name in prog_name_list:\n",
    "prog_name=input(\"Enter program name:\")\n",
    "fixed = 0\n",
    "messages = []\n",
    "\n",
    "while fixed == 0:\n",
    "    # Construct correction prompt\n",
    "    correction_prompt_value = correction_prompt_template.invoke({\n",
    "        \"prog_name\": prog_name,\n",
    "        \"messages\": \"\\n\".join(messages)\n",
    "    })\n",
    "    correction_output = run_with_retry(agent_correction_executor, correction_prompt_value.to_messages())\n",
    "    messages.append(correction_output)\n",
    "\n",
    "    # Run tests\n",
    "    if prog_name == \"knapsack\":\n",
    "        test_cmd = shell_tool.run(f\"pytest --fixed --runslow python_testcases/test_{prog_name}.py\")\n",
    "    else:\n",
    "        test_cmd = shell_tool.run(f\"pytest --fixed python_testcases/test_{prog_name}.py\")\n",
    "\n",
    "    # Construct test prompt\n",
    "    testing_prompt_value = testing_prompt_template.invoke({\"test_cmd\": test_cmd})\n",
    "    testing_output = run_with_retry(agent_testing_executor, testing_prompt_value.to_messages())\n",
    "    if testing_output == \"correct\":\n",
    "        fixed = 1\n",
    "    elif testing_output == \"wrong\":\n",
    "        fixed = 0\n",
    "\n",
    "# Construct mapping prompt\n",
    "mapping_prompt_value = mapping_prompt_template.invoke({\"logs\": ''.join(messages)})\n",
    "run_with_retry(agent_mapping_executor, mapping_prompt_value.to_messages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10183c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
